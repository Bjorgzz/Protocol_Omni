# Protocol OMNI v15.0 - Zone A: Inference Engine
# DeepSeek-V3.2 671B MoE with Blackwell GPU support
#
# CRITICAL CONSTRAINTS:
# - GPU0 (96GB): 90GB allocated = 100% utilized
# - GPU1 (32GB): 30GB allocated = 90% utilized
# - NO VRAM available for additional models
# - Blackwell reset bug mitigated via conservative probes (startupProbe 25min, livenessProbe 10min)
# - NOTE: Deployments only support restartPolicy: Always; crash recovery handled by probes
# - OOM protection: memory requests == limits to prevent overcommit
---
apiVersion: v1
kind: Namespace
metadata:
  name: inference
  labels:
    zone: brain
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-v32
  namespace: inference
  labels:
    app: deepseek-v32
    zone: brain
    tier: oracle
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: deepseek-v32
  template:
    metadata:
      labels:
        app: deepseek-v32
        zone: brain
    spec:
      runtimeClassName: nvidia
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      terminationGracePeriodSeconds: 300
      containers:
      - name: ktransformers
        image: 192.168.3.10:5000/omni/ktransformers:v15
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        command:
        - /bin/bash
        - -c
        - |
          pip install openai --quiet --break-system-packages && \
          numactl --cpunodebind=0 --interleave=all \
          python3 -m ktransformers.server.main \
          --model_path deepseek-ai/DeepSeek-V3-0324 \
          --gguf_path /models/deepseek-v3.2-dq3/Q3_K_M \
          --cpu_infer 48 \
          --host "0.0.0.0" \
          --port "8000"
        env:
        - name: ENABLE_DEEPGEMM
          value: "1"
        - name: SGLANG_ENABLE_JIT_DEEPGEMM
          value: "0"
        - name: KTRANSFORMERS_BACKEND_GPU
          value: "flashinfer"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1"
        - name: OMP_NUM_THREADS
          value: "48"
        ports:
        - containerPort: 8000
          hostPort: 8000
          protocol: TCP
        resources:
          limits:
            nvidia.com/gpu: "2"
            memory: "280Gi"
            cpu: "48"
          requests:
            nvidia.com/gpu: "2"
            memory: "280Gi"
            cpu: "24"
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
        - name: prompts
          mountPath: /prompts
          readOnly: true
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 40
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 600
          periodSeconds: 60
          timeoutSeconds: 10
          failureThreshold: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc
      - name: prompts
        persistentVolumeClaim:
          claimName: prompts-pvc
      nodeSelector:
        kubernetes.io/hostname: talos-u8k-rze
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
# GLM-4.7 CPU-Only Executor (NO GPU - VRAM exhausted by DeepSeek)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: glm-executor
  namespace: inference
  labels:
    app: glm-executor
    zone: brain
    tier: executor
spec:
  replicas: 0
  selector:
    matchLabels:
      app: glm-executor
  template:
    metadata:
      labels:
        app: glm-executor
        zone: brain
      annotations:
        deployment-mode: cpu-only
        vram-note: "GPU VRAM exhausted by DeepSeek-V3.2"
    spec:
      containers:
      - name: glm
        image: 192.168.3.10:5000/omni/ktransformers:v15
        command:
        - numactl
        - --interleave=all
        - python3
        - -m
        - ktransformers.server.main
        - --model
        - /models/glm-4.7/glm-4-9b-chat.gguf
        - --host
        - "0.0.0.0"
        - --port
        - "8002"
        - --cpu-only
        env:
        - name: ENABLE_DEEPGEMM
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: ""
        - name: OMP_NUM_THREADS
          value: "24"
        ports:
        - containerPort: 8002
          hostPort: 8002
        resources:
          limits:
            memory: "32Gi"
            cpu: "24"
          requests:
            memory: "24Gi"
            cpu: "12"
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
      volumes:
      - name: models
        hostPath:
          path: /nvme/models
          type: Directory
---
# MiniMax Failsafe - COLD STORAGE (replicas=0)
# Only scale up if DeepSeek is down
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minimax-failsafe
  namespace: inference
  labels:
    app: minimax-failsafe
    zone: brain
    tier: failsafe
  annotations:
    deployment-mode: cold-storage
    scale-policy: "emergency-only"
    warning: "Do NOT scale up while DeepSeek is running - VRAM exhaustion"
spec:
  replicas: 0
  selector:
    matchLabels:
      app: minimax-failsafe
  template:
    metadata:
      labels:
        app: minimax-failsafe
        zone: brain
    spec:
      containers:
      - name: minimax
        image: omni/vllm-blackwell:v14
        command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        - --model
        - /models/minimax-m2.1
        - --gpu-memory-utilization
        - "0.9"
        - --max-model-len
        - "32768"
        - --port
        - "8003"
        ports:
        - containerPort: 8003
          hostPort: 8003
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "64Gi"
          requests:
            nvidia.com/gpu: "1"
            memory: "32Gi"
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
      volumes:
      - name: models
        hostPath:
          path: /nvme/models
          type: Directory
