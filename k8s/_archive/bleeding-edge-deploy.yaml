# =============================================================================
# OPERATION BLEEDING EDGE: Kubernetes Deployment
# =============================================================================
# DeepSeek-V3 on Blackwell with kt-kernel + SGLang + Triton Attention
# Target: RTX 6000 Blackwell (96GB VRAM)
# CLI Args verified from: kvcache-ai/ktransformers kt-kernel/python/cli/commands/run.py
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: ai-workloads
  labels:
    pod-security.kubernetes.io/enforce: privileged
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-bleeding-edge
  namespace: ai-workloads
  labels:
    app: deepseek-bleeding-edge
    operation: bleeding-edge
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: deepseek-bleeding-edge
  template:
    metadata:
      labels:
        app: deepseek-bleeding-edge
    spec:
      runtimeClassName: nvidia
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: inference-engine
          image: 192.168.3.10:5000/omni/engine:bleeding-edge
          imagePullPolicy: IfNotPresent
          args:
            # Model Configuration
            - "--model"
            - "/models/deepseek-v3"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "30000"
            # SGLang Performance Configuration
            - "--tp"
            - "1"
            - "--mem-fraction-static"
            - "0.90"
            - "--max-total-tokens"
            - "32768"
            - "--max-running-requests"
            - "8"
            - "--chunked-prefill-size"
            - "8192"
            - "--trust-remote-code"
            # Attention: triton (Blackwell SM_100 lacks FlashInfer kernels)
            - "--attention-backend"
            - "triton"
          env:
            # Pin to Blackwell GPU (96GB)
            - name: NVIDIA_VISIBLE_DEVICES
              value: "GPU-f4f210c1-5a52-7267-979e-fe922961190a"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Memory optimization
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "expandable_segments:True,max_split_size_mb:512"
            # Triton cache
            - name: TRITON_CACHE_DIR
              value: "/models/.triton_cache"
            # HuggingFace cache
            - name: HF_HOME
              value: "/models"
            - name: TRANSFORMERS_CACHE
              value: "/models"
          ports:
            - name: http
              containerPort: 30000
              protocol: TCP
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "128Gi"
              cpu: "48"
            requests:
              nvidia.com/gpu: "1"
              memory: "64Gi"
              cpu: "24"
          volumeMounts:
            - name: model-storage
              mountPath: /models
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 30000
            initialDelaySeconds: 600
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 30000
            initialDelaySeconds: 300
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 10
      volumes:
        - name: model-storage
          hostPath:
            path: /var/mnt/data/models
            type: DirectoryOrCreate
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
---
apiVersion: v1
kind: Service
metadata:
  name: deepseek-bleeding-edge
  namespace: ai-workloads
spec:
  type: NodePort
  selector:
    app: deepseek-bleeding-edge
  ports:
    - name: http
      port: 30000
      targetPort: 30000
      nodePort: 30000
      protocol: TCP
---
# =============================================================================
# Flux/ComfyUI on RTX 5090 (Image Generation)
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flux-bleeding-edge
  namespace: ai-workloads
  labels:
    app: flux-bleeding-edge
    operation: bleeding-edge
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: flux-bleeding-edge
  template:
    metadata:
      labels:
        app: flux-bleeding-edge
    spec:
      runtimeClassName: nvidia
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: comfyui
          image: ghcr.io/ai-dock/comfyui:latest
          imagePullPolicy: Always
          env:
            # Pin to RTX 5090 (32GB)
            - name: NVIDIA_VISIBLE_DEVICES
              value: "GPU-bfbb9aa1-3d36-b47b-988f-5752cfc54601"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
          ports:
            - name: http
              containerPort: 8188
              protocol: TCP
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "48Gi"
              cpu: "16"
            requests:
              nvidia.com/gpu: "1"
              memory: "32Gi"
              cpu: "8"
          volumeMounts:
            - name: model-storage
              mountPath: /workspace/ComfyUI/models
      volumes:
        - name: model-storage
          hostPath:
            path: /var/mnt/data/comfyui-models
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: flux-bleeding-edge
  namespace: ai-workloads
spec:
  type: NodePort
  selector:
    app: flux-bleeding-edge
  ports:
    - name: http
      port: 8188
      targetPort: 8188
      nodePort: 30188
      protocol: TCP
