# Protocol OMNI v15.0 - Prometheus Alert Rules
# Critical alerts for Blackwell GPU stability, memory pressure, and inference health
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: monitoring
  labels:
    app: prometheus
data:
  omni-alerts.yaml: |
    groups:
    - name: omni-critical
      interval: 30s
      rules:
      # Blackwell Reset Loop Detection
      - alert: BlackwellResetLoopRisk
        expr: increase(kube_pod_container_status_restarts_total{container="ktransformers"}[10m]) > 2
        for: 1m
        labels:
          severity: critical
          zone: brain
        annotations:
          summary: "GPU pod restart loop detected - Blackwell FLR bug triggered"
          description: "Container {{ $labels.container }} in pod {{ $labels.pod }} has restarted {{ $value }} times in 10 minutes"
          action: "kubectl scale deployment inference-engine --replicas=0 && notify-oncall"
          runbook: "https://docs.omni/runbooks/blackwell-reset-loop"

      # High Memory Pressure (approaching swap)
      - alert: HighMemoryPressure
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.92
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Memory usage above 92% - approaching swap threshold"
          description: "Node {{ $labels.instance }} memory at {{ $value | humanizePercentage }}"
          action: "Check for memory leaks, consider reducing batch size"

      # Critical Memory (swap in use)
      - alert: CriticalMemoryPressure
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.96
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Memory usage above 96% - swap actively in use"
          description: "Node {{ $labels.instance }} memory at {{ $value | humanizePercentage }}, OOM risk imminent"
          action: "Reduce load immediately or scale down non-essential services"

      # Swap Usage Detection
      - alert: SwapInUse
        expr: node_memory_SwapFree_bytes < node_memory_SwapTotal_bytes * 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Swap usage above 50%"
          description: "{{ $value | humanize }} swap remaining on {{ $labels.instance }}"
          action: "Memory pressure detected - performance degraded"

      # Swap Nearly Exhausted
      - alert: SwapNearlyExhausted
        expr: node_memory_SwapFree_bytes < node_memory_SwapTotal_bytes * 0.1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Swap usage above 90% - OOM imminent"
          description: "Only {{ $value | humanize }} swap remaining"
          action: "Emergency: Scale down workloads NOW"

    - name: omni-inference
      interval: 30s
      rules:
      # Low Inference Throughput
      - alert: LowInferenceThroughput
        expr: inference_tokens_per_second < 5
        for: 5m
        labels:
          severity: warning
          zone: brain
        annotations:
          summary: "Inference throughput below 5 tok/s"
          description: "Current throughput: {{ $value }} tok/s"
          action: "Check GPU utilization and KV cache pressure"

      # Inference Latency High
      - alert: HighInferenceLatency
        expr: histogram_quantile(0.95, rate(inference_request_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
          zone: brain
        annotations:
          summary: "P95 inference latency above 30s"
          description: "P95 latency: {{ $value }}s"

      # Inference Service Down
      - alert: InferenceServiceDown
        expr: up{job="deepseek-v32"} == 0
        for: 2m
        labels:
          severity: critical
          zone: brain
        annotations:
          summary: "DeepSeek-V3.2 inference service is down"
          action: "Check pod status, GPU health, and logs"

    - name: omni-gpu
      interval: 30s
      rules:
      # High GPU Memory
      - alert: HighGPUMemory
        expr: DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL > 0.98
        for: 5m
        labels:
          severity: warning
          zone: brain
        annotations:
          summary: "GPU memory above 98%"
          description: "GPU {{ $labels.gpu }} at {{ $value | humanizePercentage }} VRAM"

      # GPU Temperature High
      - alert: HighGPUTemperature
        expr: DCGM_FI_DEV_GPU_TEMP > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU temperature above 85°C"
          description: "GPU {{ $labels.gpu }} at {{ $value }}°C"
          action: "Check cooling, consider reducing load"

      # GPU Throttling
      - alert: GPUThermalThrottling
        expr: DCGM_FI_DEV_GPU_TEMP > 90
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "GPU thermal throttling likely"
          description: "GPU {{ $labels.gpu }} at {{ $value }}°C - performance degraded"

    - name: omni-agents
      interval: 30s
      rules:
      # Agent Service Down
      - alert: AgentOrchestratorDown
        expr: up{job="agent-orchestrator"} == 0
        for: 2m
        labels:
          severity: critical
          zone: hands
        annotations:
          summary: "Agent Orchestrator is down"
          action: "Check gVisor pod status and logs"

      # Metacognition Service Down
      - alert: MetacognitionDown
        expr: up{job="metacognition"} == 0
        for: 2m
        labels:
          severity: warning
          zone: hands
        annotations:
          summary: "Metacognition service is down"
          description: "Verification gates inactive - confabulation risk"

    - name: omni-memory
      interval: 60s
      rules:
      # Letta Down
      - alert: LettaDown
        expr: up{job="letta"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Letta memory service is down"

      # Qdrant Down
      - alert: QdrantDown
        expr: up{job="qdrant"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Qdrant vector store is down"

      # Memgraph Down
      - alert: MemgraphDown
        expr: up{job="memgraph"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Memgraph knowledge graph is down"
